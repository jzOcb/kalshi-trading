import warnings; warnings.filterwarnings("ignore", message="urllib3 v2")
"""
Kalshi Enhanced Report with Decision Engine
Scans markets â†’ Analyzes rules â†’ Makes BUY/WAIT/SKIP recommendations
"""
import sys

try:
    import requests
except ImportError:
    import urllib.request
    import urllib.parse
    import json as _json
    print("âš ï¸ requests not available, using urllib fallback", file=sys.stderr)
    
    class _Response:
        def __init__(self, status, body):
            self.status_code = status
            self._body = body
            self.text = body.decode('utf-8')  # Add .text attribute
        def json(self):
            return _json.loads(self._body.decode('utf-8'))
        def raise_for_status(self):
            if not (200 <= self.status_code < 300):
                raise Exception(f"HTTP {self.status_code}")
    
    class requests:
        @staticmethod
        def get(url, params=None, timeout=15):
            if params:
                url = url + "?" + urllib.parse.urlencode(params)
            req = urllib.request.Request(url)
            with urllib.request.urlopen(req, timeout=timeout) as response:
                body = response.read()
                return _Response(response.status, body)
import json
import os
import re
import time
from datetime import datetime, timezone, timedelta

API_BASE = "https://api.elections.kalshi.com/trade-api/v2"
WATCHLIST_FILE = os.path.join(os.path.dirname(__file__), "data", "watchlist_series.json")

# Fallback ç¡¬ç¼–ç åˆ—è¡¨ (å½“ watchlist ä¸å­˜åœ¨æ—¶ä½¿ç”¨)
FALLBACK_SERIES = [
    "KXGDP", "KXCPI", "KXFED", "KXPCE", "KXJOBLESS", "KXUNEMPLOY",
    "KXFOMC", "KXRATECUTCOUNT", "KXAAGAS", "KXGASMAX", "KXGASAVG",
    "KXSHUTDOWN", "KXDHSFUND", "KXDEBT", "KXTARIFF", "KXRECESSION",
    "KXCR", "KXEOWEEK", "KXEOTRUMPTERM", "KXBILLSIGNED", "KXCABINET",
]

def load_watchlist_series():
    """ä» watchlist_series.json åŠ è½½ series åˆ—è¡¨"""
    try:
        if os.path.exists(WATCHLIST_FILE):
            with open(WATCHLIST_FILE) as f:
                data = json.load(f)
            series = data.get("series", [])
            if series:
                print(f"ğŸ“‹ ä» watchlist åŠ è½½ {len(series)} ä¸ª series", file=sys.stderr)
                return series
    except Exception as e:
        print(f"âš ï¸ è¯»å– watchlist å¤±è´¥: {e}", file=sys.stderr)
    
    print(f"ğŸ“‹ ä½¿ç”¨ fallback series ({len(FALLBACK_SERIES)} ä¸ª)", file=sys.stderr)
    return FALLBACK_SERIES

def api_get(endpoint, params=None):
    try:
        resp = requests.get(f"{API_BASE}{endpoint}", params=params, timeout=15)
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        # print(f"API error on {endpoint}: {e}")  # Suppress noise
        return None

def fetch_market_details(ticker):
    """Fetch complete market details including rules"""
    data = api_get(f"/markets/{ticker}")
    if not data:
        return None
    return data.get("market", {})

def kalshi_url(ticker):
    return f"https://kalshi.com/markets/{ticker.lower()}"

def search_polymarket(query, max_results=3):
    """Search Polymarket for matching markets, return YES probability (0-1) or None"""
    try:
        r = requests.get("https://gamma-api.polymarket.com/events",
                        params={"active": "true", "closed": "false", "limit": 20},
                        timeout=10)
        if r.status_code != 200:
            return None
        data = r.json()
        events = data if isinstance(data, list) else []
        # Fuzzy match by title keywords
        query_lower = query.lower()
        for event in events:
            title = event.get("title", "").lower()
            # Check if 2+ keywords match
            keywords = [w for w in query_lower.split() if len(w) > 3]
            matches = sum(1 for kw in keywords if kw in title)
            if matches >= 2:
                markets = event.get("markets", [])
                if markets:
                    # Get the main market price (0-1 range)
                    prices = json.loads(markets[0].get("outcomePrices", "[]"))
                    if prices:
                        return float(prices[0])
        return None
    except:
        return None


def search_news(query, max_results=5):
    """Search Google News RSS for recent articles"""
    results = []
    try:
        import urllib.parse
        query_encoded = urllib.parse.quote(query)
        url = f"https://news.google.com/rss/search?q={query_encoded}&hl=en-US&gl=US&ceid=US:en"
        
        if hasattr(requests, 'get'):
            # Using requests
            r = requests.get(url, timeout=10)
            text = r.text
        else:
            # Using urllib fallback
            import urllib.request
            req = urllib.request.Request(url)
            with urllib.request.urlopen(req, timeout=10) as resp:
                text = resp.read().decode('utf-8')
        
        import re
        titles = re.findall(r'<title>(.*?)</title>', text)
        dates = re.findall(r'<pubDate>(.*?)</pubDate>', text)
        
        for i, title in enumerate(titles[1:max_results+1]):  # skip feed title
            results.append({
                "title": title,
                "date": dates[i] if i < len(dates) else "",
            })
    except Exception as e:
        # Fail silently, don't block on news errors
        pass
    return results

def format_vol(v):
    if v >= 1_000_000:
        return f"{v/1_000_000:.1f}M"
    elif v >= 1_000:
        return f"{v/1_000:.0f}K"
    return str(v)

def analyze_rules(rules_text):
    """Parse resolution rules"""
    analysis = {
        "official_source": None,
        "procedural_risk": False,
        "time_window": None,
        "ambiguity": False,
    }
    
    if not rules_text:
        analysis["ambiguity"] = True
        return analysis
    
    text_lower = rules_text.lower()
    
    # Official data sources
    sources = {
        "BEA": ["bureau of economic analysis", "bea.gov", "bea's", " bea ", "gdp release"],
        "BLS": ["bureau of labor statistics", "bls.gov", "bls's", " bls ", "cpi release"],
        "Fed": ["federal reserve", "fomc", "fed.gov", "interest rate decision"],
        "Congress": ["congress.gov", "congressional", "legislative", "house.gov", "senate.gov"],
        "White House": ["whitehouse.gov", "executive order", "presidential", "president signs"],
        "Treasury": ["treasury.gov", "treasury department"],
    }
    
    for source, keywords in sources.items():
        if any(kw in text_lower for kw in keywords):
            analysis["official_source"] = source
            break
    
    # Implicit sources (inferred from indicators mentioned in rules)
    if not analysis["official_source"]:
        # CPI â†’ BLS
        if "consumer price index" in text_lower or " cpi " in text_lower:
            analysis["official_source"] = "BLS"
        # GDP â†’ BEA
        elif " gdp " in text_lower or "gross domestic product" in text_lower or "real gdp" in text_lower:
            analysis["official_source"] = "BEA"
        # Unemployment â†’ BLS
        elif "unemployment" in text_lower or "jobs report" in text_lower:
            analysis["official_source"] = "BLS"
        # Interest rate â†’ Fed
        elif "interest rate" in text_lower or "federal funds rate" in text_lower:
            analysis["official_source"] = "Fed"
    
    # Procedural complexity
    procedural_keywords = [
        "pass both", "senate and house", "signed into law",
        "confirmed by", "ratified", "approved by congress",
    ]
    if any(kw in text_lower for kw in procedural_keywords):
        analysis["procedural_risk"] = True
    
    # Ambiguous terms
    ambiguous_terms = ["may", "could", "might", "approximately", "around"]
    if any(term in text_lower for term in ambiguous_terms):
        analysis["ambiguity"] = True
    
    return analysis

def score_market(m):
    """
    Score and decide on a market.
    
    NEW PHILOSOPHY (2026-02-20 GDP lesson):
    äº‹å®æ ¸æŸ¥ä¼˜å…ˆï¼Œæ”¶ç›Šç‡æ˜¯æ¬¡è¦çš„ã€‚
    
    Tier 1: å¯æ ¸æŸ¥æ€§ (GATE - ä¸é€šè¿‡ç›´æ¥è·³è¿‡)
    Tier 2: æ–¹å‘ç¡®å®šæ€§ (å†³å®šæ˜¯å¦æ¨è)
    Tier 3: æ”¶ç›Šç‡ (åªå½±å“ä»“ä½å¤§å°)
    """
    reasons = []
    warnings = []
    
    price = m.get("last_price", 50)
    spread = (m.get("yes_ask", 0) - m.get("yes_bid", 0)) if m.get("yes_ask") else 99
    ticker = m.get("ticker", "").upper()
    title_raw = m.get("title", "")
    title = title_raw.lower()
    
    rules_primary = m.get("rules_primary", "")
    rules_secondary = m.get("rules_secondary", "")
    rules = f"{rules_primary} {rules_secondary}"
    text_lower = f"{title} {rules}".lower()
    
    close_str = m.get("close_time", "")
    if not close_str:
        return None
    
    try:
        close = datetime.fromisoformat(close_str.replace("Z", "+00:00"))
        days = (close - datetime.now(timezone.utc)).days
    except:
        return None
    
    if days <= 0:
        return None
    
    side = "YES" if price >= 85 else "NO"
    cost = price if price >= 85 else (100 - price)
    ret = ((100 - cost) / cost) * 100 if cost > 0 else 0
    ann_yield = (ret / max(days, 1)) * 365
    
    if ann_yield < 100:
        return None
    
    # ================================================================
    # TIER 1: å¯æ ¸æŸ¥æ€§ (VERIFIABILITY GATE)
    # é—®é¢˜ï¼šè¿™ä¸ªå¸‚åœºçš„ç»“æœèƒ½å¦è¢«å®¢è§‚æ•°æ®éªŒè¯ï¼Ÿ
    # ä¸èƒ½éªŒè¯ = çº¯èµŒåš = ç›´æ¥è·³è¿‡
    # ================================================================
    
    verifiability_score = 0  # 0-100
    rule_analysis = analyze_rules(rules)
    
    # æœ‰å®˜æ–¹æ•°æ®æº = å¯æ ¸æŸ¥
    if rule_analysis["official_source"]:
        verifiability_score += 50
        reasons.append(f"âœ… {rule_analysis['official_source']} æ•°æ®æº")
    else:
        # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å¯æ ¸æŸ¥æ€§æŒ‡æ ‡
        verifiable_keywords = [
            ("price", "stock", "index", "s&p", "nasdaq", "dow"),  # é‡‘èæ•°æ®
            ("temperature", "weather", "rain", "snow"),  # å¤©æ°”æ•°æ®
            ("rate", "fed", "central bank", "pboc", "ecb"),  # å¤®è¡Œå†³ç­–
            ("election", "vote", "poll"),  # é€‰ä¸¾ç»“æœ
        ]
        for keywords in verifiable_keywords:
            if any(kw in text_lower for kw in keywords):
                verifiability_score += 30
                reasons.append("âš ï¸ æœ‰å®¢è§‚æ•°æ®ä½†éå®˜æ–¹æº")
                break
        else:
            reasons.append("âŒ æ— å¯æ ¸æŸ¥æ•°æ®æº")
    
    # "Trump è¯´ X" ç±»å¸‚åœº - éš¾ä»¥æå‰æ ¸æŸ¥
    if "trump" in text_lower and "say" in text_lower:
        verifiability_score -= 40
        warnings.append("ğŸ”´ 'Trumpè¯´'ç±»å¸‚åœºæ— æ³•æå‰æ ¸æŸ¥")
    
    # è§„åˆ™æ¨¡ç³Š = æ ¸æŸ¥å›°éš¾
    if rule_analysis["ambiguity"]:
        verifiability_score -= 20
        warnings.append("âš ï¸ è§„åˆ™æ¨¡ç³Šï¼Œæ ¸æŸ¥å›°éš¾")
    
    # ç¨‹åºæ€§é£é™© = ç»“æœä¸ç¡®å®š
    if rule_analysis["procedural_risk"]:
        verifiability_score -= 15
        warnings.append("âš ï¸ æœ‰ç¨‹åºæ€§éšœç¢")
    
    # ================================================================
    # GATE CHECK: å¯æ ¸æŸ¥æ€§å¤ªä½ = è·³è¿‡
    # ================================================================
    if verifiability_score < 20:
        return {
            "decision": "ğŸ”´ SKIP",
            "score": verifiability_score,
            "confidence": "UNVERIFIABLE",
            "position": 0,
            "side": side,
            "cost": cost,
            "ann_yield": ann_yield,
            "days": days,
            "reasons": reasons + warnings,
            "warnings": warnings,
            "spread": spread,
            "vol": m.get("volume_24h", 0),
            "ticker": ticker,
            "title": title_raw,
            "sub": m.get("yes_sub_title", "") or m.get("no_sub_title", ""),
            "pm_price": None,
            "skip_reason": "æ— æ³•äº‹å®æ ¸æŸ¥",
        }
    
    # ================================================================
    # TIER 2: æ–¹å‘ç¡®å®šæ€§ (DIRECTION CONFIDENCE)
    # é—®é¢˜ï¼šæˆ‘ä»¬èƒ½å¦åˆ¤æ–­ç»“æœæ›´å¯èƒ½æ˜¯ YES è¿˜æ˜¯ NOï¼Ÿ
    # ================================================================
    
    direction_score = 0  # 0-100
    
    # Nowcast/æ¨¡å‹ä¾èµ– = æ–¹å‘ä¸ç¡®å®š
    is_nowcast_market = False
    if "GDP" in ticker or "gdp" in title:
        is_nowcast_market = True
        direction_score -= 30
        warnings.append("âš ï¸ GDPä¾èµ–GDPNow(Q4è¯¯å·®2.8pp)")
    if "CPI" in ticker or "cpi" in title or "inflation" in title:
        is_nowcast_market = True
        direction_score -= 20
        warnings.append("âš ï¸ CPIä¾èµ–Nowcastæ¨¡å‹")
    
    # æ”¿ç­–äº‹ä»¶é£é™© = æ–¹å‘ä¸ç¡®å®š
    policy_keywords = ["shutdown", "tariff", "trade war", "debt ceiling", "impeach"]
    for kw in policy_keywords:
        if kw in text_lower:
            direction_score -= 25
            warnings.append(f"âš ï¸ æ”¿ç­–äº‹ä»¶({kw})å½±å“æ–¹å‘")
            break
    
    # é«˜ä»·å…¥åœº = å®¹é”™ç©ºé—´å°
    if cost >= 90:
        direction_score -= 30
        warnings.append(f"ğŸ”´ å…¥åœº{cost}Â¢ï¼Œé”™äº†äº95%+")
    elif cost >= 85:
        direction_score -= 15
        warnings.append(f"âš ï¸ å…¥åœº{cost}Â¢ï¼Œå®¹é”™ç©ºé—´å°")
    
    # Nowcast + é«˜ä»· = GDPæ•™è®­ç»„åˆ
    if is_nowcast_market and cost >= 85:
        direction_score -= 40
        warnings.append("ğŸ”´ Nowcast+é«˜ä»·=GDPæ•™è®­(äº$179)")
    
    # ================================================================
    # TIER 3: æ”¶ç›Šç‡ (YIELD - åªå½±å“ä»“ä½)
    # åªæœ‰é€šè¿‡ Tier 1 & 2 æ‰è€ƒè™‘æ”¶ç›Šç‡
    # ================================================================
    
    # æµåŠ¨æ€§æ£€æŸ¥
    liquidity_ok = spread <= 5
    if spread <= 2:
        reasons.append("æµåŠ¨æ€§ä¼˜")
    elif spread <= 5:
        reasons.append("æµåŠ¨æ€§å¯")
    else:
        warnings.append("âš ï¸ æµåŠ¨æ€§å·®")
        liquidity_ok = False
    
    reasons.append(f"å¹´åŒ– {ann_yield:.0f}%")
    
    # ================================================================
    # ç»¼åˆå†³ç­–
    # ================================================================
    
    # è®¡ç®—æœ€ç»ˆå¾—åˆ† (äº‹å®æ ¸æŸ¥ä¸ºä¸»)
    # æƒé‡: å¯æ ¸æŸ¥æ€§ 60% + æ–¹å‘ç¡®å®šæ€§ 40%
    final_score = (verifiability_score * 0.6) + (direction_score * 0.4) + 50  # +50 baseline
    
    # æ”¶ç›Šç‡åªåŠ å°‘é‡åˆ†æ•° (æœ€å¤š+20)
    yield_bonus = min(ann_yield / 100, 20)
    final_score += yield_bonus
    
    # å†³ç­–
    if final_score >= 70 and liquidity_ok and direction_score >= -20:
        decision = "ğŸŸ¢ BUY"
        confidence = "HIGH"
        position = 100 if final_score >= 85 else 50
    elif final_score >= 50 and direction_score >= -40:
        decision = "ğŸŸ¡ WAIT"
        confidence = "MEDIUM"
        position = 25
    else:
        decision = "ğŸ”´ SKIP"
        confidence = "LOW"
        position = 0
    
    # News validation (only for promising candidates)
    news_count = 0
    pm_price_val = None
    if final_score >= 50:  # Only search news for candidates worth considering
        title_for_search = m.get("title", "")
        subtitle = m.get("yes_sub_title", "") or m.get("no_sub_title", "")
        
        # Build search query from title keywords
        query_terms = []
        # Extract key terms from title
        import re
        # Clean markdown and special characters first
        title_clean = re.sub(r'\*\*', '', title)  # Remove markdown bold
        title_clean = re.sub(r'[^\w\s]', ' ', title_clean)  # Remove punctuation
        # Remove common words and extract important terms
        title_clean = re.sub(r'\b(will|the|a|an|in|on|at|to|for|of|by|more|than|less|increase|decrease)\b', '', title_clean.lower())
        terms = [t.strip() for t in title_clean.split() if len(t.strip()) > 2][:3]
        query_terms.extend(terms)
        
        if query_terms:
            query = " ".join(query_terms)
            news_results = search_news(query, max_results=5)
            news_count = len([n for n in news_results if n.get("title")])
            
            if news_count >= 3:
                direction_score += 10  # æ–°é—»éªŒè¯å¢åŠ æ–¹å‘ç¡®å®šæ€§
                reasons.append(f"âœ… {news_count}æ¡æ–°é—»ä½è¯")
            elif news_count > 0:
                reasons.append(f"âš ï¸ ä»…{news_count}æ¡æ–°é—»")
            else:
                direction_score -= 10
                warnings.append("âŒ æ— ç›¸å…³æ–°é—»ä½è¯")
        
        time.sleep(0.1)

        # Polymarket cross-validation
        if query_terms:
            pm_price_val = search_polymarket(query)
            if pm_price_val is not None:
                kalshi_prob = price / 100
                gap = abs(pm_price_val - kalshi_prob)
                if gap < 0.05:
                    direction_score += 15  # å¸‚åœºå…±è¯†å¢åŠ æ–¹å‘ç¡®å®šæ€§
                    reasons.append(f"âœ… Polymarket {pm_price_val:.0%} ä¸€è‡´")
                elif gap > 0.15:
                    warnings.append(f"âš ï¸ Polymarket {pm_price_val:.0%} åå·®å¤§")

    # é‡æ–°è®¡ç®—æœ€ç»ˆå¾—åˆ†ï¼ˆåŒ…å«æ–°é—»/PolymarketéªŒè¯åçš„direction_scoreï¼‰
    final_score = (verifiability_score * 0.6) + (direction_score * 0.4) + 50
    final_score += min(ann_yield / 100, 20)  # æ”¶ç›Šç‡åªåŠ å°‘é‡åˆ†
    
    # æœ€ç»ˆå†³ç­–
    if final_score >= 70 and liquidity_ok and direction_score >= -20:
        decision = "ğŸŸ¢ BUY"
        confidence = "HIGH"
        position = 100 if final_score >= 85 else 50
    elif final_score >= 50 and direction_score >= -40:
        decision = "ğŸŸ¡ WAIT"
        confidence = "MEDIUM"
        position = 25
    else:
        decision = "ğŸ”´ SKIP"
        confidence = "LOW"
        position = 0
    
    all_reasons = reasons + warnings
    
    return {
        "decision": decision,
        "score": int(final_score),
        "confidence": confidence,
        "position": position,
        "side": side,
        "cost": cost,
        "ann_yield": ann_yield,
        "days": days,
        "reasons": all_reasons,
        "warnings": warnings,  # Separate for filtering
        "spread": spread,
        "vol": m.get("volume_24h", 0),
        "ticker": m.get("ticker", ""),
        "title": m.get("title", ""),
        "sub": m.get("yes_sub_title", "") or m.get("no_sub_title", ""),
        "pm_price": pm_price_val,
    }

def scan_and_decide():
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    now = datetime.now(timezone.utc)
    
    # === OPTIMIZATION CONFIG ===
    MIN_VOLUME = 200  # Skip low liquidity markets
    MAX_WORKERS_SERIES = 3   # Conservative: Kalshi rate-limits at 8+ concurrent (429)
    MAX_WORKERS_DETAILS = 5  # Parallel detail fetches
    
    # Step 1: Fetch ALL non-sports markets via Events API (expanded coverage)
    print(f"Scanning ALL non-sports markets via Events API...", file=sys.stderr, flush=True)
    all_markets = []
    
    def fetch_all_events():
        """Fetch all non-sports markets via events API"""
        import requests as _req
        markets = []
        cursor = None
        for page in range(30):
            params = {'limit': 100, 'status': 'open', 'with_nested_markets': 'true'}
            if cursor:
                params['cursor'] = cursor
            try:
                resp = _req.get(f"{API_BASE}/events", params=params, timeout=15)
                if resp.status_code == 429:
                    time.sleep(2)
                    continue
                if resp.status_code != 200:
                    break
                data = resp.json()
                for e in data.get('events', []):
                    cat = e.get('category', '')
                    if cat not in ['Sports', 'Entertainment']:
                        markets.extend(e.get('markets', []))
                cursor = data.get('cursor')
                if not cursor or len(data.get('events', [])) < 100:
                    break
                if page % 5 == 0:
                    print(f"  Page {page}: {len(markets)} markets so far...", file=sys.stderr, flush=True)
            except Exception as ex:
                print(f"  Events API error: {ex}", file=sys.stderr)
                break
        return markets
    
    all_markets = fetch_all_events()
    print(f"  Loaded {len(all_markets)} non-sports markets", file=sys.stderr, flush=True)
    
    # Step 2: Filter candidates (extreme price + volume filter)
    candidates = []
    filtered_low_vol = 0
    for m in all_markets:
        price = m.get("last_price", 50)
        volume = m.get("volume_24h", 0) or m.get("volume", 0)
        
        # Skip low volume markets (optimization)
        if volume < MIN_VOLUME:
            filtered_low_vol += 1
            continue
            
        if (price >= 85 or price <= 12):
            candidates.append(m)
    
    print(f"Found {len(candidates)} candidates from {len(all_markets)} markets (filtered {filtered_low_vol} low-vol)", file=sys.stderr)
    
    # Step 3: Fetch detailed rules (PARALLEL)
    print(f"Analyzing {len(candidates)} candidates (parallel)...", file=sys.stderr, flush=True)
    
    def analyze_candidate(m):
        ticker = m.get("ticker", "")
        detailed = fetch_market_details(ticker)
        if detailed:
            m["rules_primary"] = detailed.get("rules_primary", "")
            m["rules_secondary"] = detailed.get("rules_secondary", "")
            return score_market(m)
        return None
    
    opportunities = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS_DETAILS) as executor:
        futures = {executor.submit(analyze_candidate, m): m for m in candidates}
        done = 0
        for future in as_completed(futures):
            result = future.result()
            if result:
                opportunities.append(result)
            done += 1
            if done % 20 == 0 or done == len(candidates):
                print(f"  Progress: {done}/{len(candidates)} analyzed", file=sys.stderr, flush=True)
    
    # Sort by score
    opportunities.sort(key=lambda x: -x["score"])
    
    # Load existing positions from both accounts
    positions_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), "positions.json")
    existing_positions = {}  # ticker -> {side, qty, account}
    try:
        with open(positions_file, "r") as f:
            pos_data = json.load(f)
            for p in pos_data.get("positions", []):
                ticker = p.get("ticker", "")
                existing_positions[ticker] = {
                    "side": p.get("side"),
                    "qty": p.get("contracts", 0),
                    "account": p.get("account", "ä¸»è´¦å·"),
                    "entry": p.get("entry_price", 0)
                }
    except Exception as e:
        print(f"âš ï¸ Could not load positions: {e}", file=sys.stderr)
    
    # Format report
    lines = []
    lines.append(f"âš¡ Kalshi Decision Report â€” {now.strftime('%m/%d %H:%M UTC')}")
    lines.append(f"æ‰«æäº† {len(all_markets)} ä¸ªå¸‚åœºï¼Œæ‰¾åˆ° {len(opportunities)} ä¸ªé«˜ç¡®å®šæ€§æœºä¼š\n")
    
    if not opportunities:
        lines.append("ğŸ˜´ æš‚æ— ç¬¦åˆæ ‡å‡†çš„æœºä¼š")
        return "\n".join(lines)
    
    # Categorize
    buys = [o for o in opportunities if "BUY" in o["decision"]]
    waits = [o for o in opportunities if "WAIT" in o["decision"]]
    skips = [o for o in opportunities if "SKIP" in o["decision"]]
    
    # BUY recommendations
    if buys:
        lines.append(f"ğŸŸ¢ æ¨èä¹°å…¥ ({len(buys)})\n")
        for i, o in enumerate(buys[:5], 1):
            full_name = f"{o['title']} â†’ {o['sub']}" if o['sub'] else o['title']
            ticker = o['ticker']
            
            # Check if already have position
            pos_info = existing_positions.get(ticker)
            pos_tag = ""
            if pos_info:
                pos_tag = f" ğŸ“Œ å·²æŒæœ‰ {pos_info['qty']}å¼ {pos_info['side']}@{pos_info['entry']}Â¢ ({pos_info['account']})"
            
            pm_tag = f" | PM {o['pm_price']:.0%}" if o.get("pm_price") is not None else ""
            lines.append(f"#{i} {o['decision']} â€” è¯„åˆ† {o['score']}/100{pos_tag}")
            lines.append(f"   {full_name}")
            lines.append(f"   ğŸ‘‰ {o['side']} @ {o['cost']:.0f}Â¢ | ä»“ä½ ${o['position']}{pm_tag}")
            lines.append(f"   ğŸ“Š {o['ann_yield']:.0f}% å¹´åŒ– ({o['days']}å¤©) | spread {o['spread']}Â¢ | é‡ {format_vol(o['vol'])}")
            lines.append(f"   ğŸ’¡ {' | '.join(o['reasons'])}")
            lines.append(f"   ğŸ”— {kalshi_url(o['ticker'])}\n")
    
    # WAIT candidates
    if waits:
        lines.append(f"ğŸŸ¡ è§‚æœ›ä¸­ ({len(waits)})\n")
        for o in waits[:3]:
            full_name = f"{o['title']} â†’ {o['sub']}" if o['sub'] else o['title']
            pm_tag = f" | PM {o['pm_price']:.0%}" if o.get("pm_price") is not None else ""
            lines.append(f"   {o['decision']} ({o['score']}/100) â€” {full_name}")
            lines.append(f"   {o['side']} @ {o['cost']:.0f}Â¢ | {o['ann_yield']:.0f}% ann{pm_tag}")
            lines.append(f"   ğŸ’¡ {' | '.join(o['reasons'])}")
            lines.append(f"   {kalshi_url(o['ticker'])}\n")
    
    # SKIP (show why they were rejected)
    if skips and not buys and not waits:
        lines.append(f"ğŸ”´ å·²æ‹’ç» ({len(skips)}) â€” é«˜æ”¶ç›Šä½†é£é™©ä¸å¯æ§\n")
        for o in skips[:3]:
            full_name = f"{o['title']} â†’ {o['sub']}" if o['sub'] else o['title']
            lines.append(f"   SKIP ({o['score']}/100) â€” {full_name}")
            lines.append(f"   {o['side']} @ {o['cost']:.0f}Â¢ | {o['ann_yield']:.0f}% ann")
            lines.append(f"   âŒ æ‹’ç»åŸå› : {' | '.join(o['reasons'])}")
            lines.append(f"   {kalshi_url(o['ticker'])}\n")
    
    if not buys and not waits:
        lines.append("\nâš ï¸ æœ¬è½®æ‰«ææ— æ¨èæ ‡çš„ â€” æ‰€æœ‰é«˜æ”¶ç›Šæœºä¼šéƒ½å› è§„åˆ™/æ•°æ®æºé—®é¢˜è¢«æ‹’ç»")
    
    return "\n".join(lines)

if __name__ == "__main__":
    report = scan_and_decide()
    print(report)
