import warnings; warnings.filterwarnings("ignore", message="urllib3 v2")
"""
Kalshi Enhanced Report with Decision Engine
Scans markets â†’ Analyzes rules â†’ Makes BUY/WAIT/SKIP recommendations
"""
import sys

try:
    import requests
except ImportError:
    import urllib.request
    import urllib.parse
    import json as _json
    print("âš ï¸ requests not available, using urllib fallback", file=sys.stderr)
    
    class _Response:
        def __init__(self, status, body):
            self.status_code = status
            self._body = body
            self.text = body.decode('utf-8')  # Add .text attribute
        def json(self):
            return _json.loads(self._body.decode('utf-8'))
        def raise_for_status(self):
            if not (200 <= self.status_code < 300):
                raise Exception(f"HTTP {self.status_code}")
    
    class requests:
        @staticmethod
        def get(url, params=None, timeout=15):
            if params:
                url = url + "?" + urllib.parse.urlencode(params)
            req = urllib.request.Request(url)
            with urllib.request.urlopen(req, timeout=timeout) as response:
                body = response.read()
                return _Response(response.status, body)
import json
import os
import re
import time
from datetime import datetime, timezone, timedelta

API_BASE = "https://api.elections.kalshi.com/trade-api/v2"

POLITICAL_SERIES = [
    "KXGDP", "KXCPI", "KXFED", "KXGOVSHUTLENGTH",
    "KXTRUMPMEETING", "KXGREENLAND", "KXSCOTUS", "KXRECESSION",
    "KXUKRAINE", "KXIRAN", "KXBITCOIN", "KXSP500", "KXDOGE",
    "KXGOVTCUTS", "KXGOVTSPEND", "KXDEBT", "KXCR",
    "KXSHUTDOWNBY", "KXTARIFF", "KXFEDRATE",
    "KXCABINET", "KXTERMINALRATE", "KXLOWESTRATE",
    "KXTRUMPSAYNICKNAME", "KXTRUMPRESIGN", "KXTRUMPREMOVE",
    "KXTRUMPPARDONFAMILY", "KXTRUMPAGCOUNT", "KXEOTRUMPTERM",
    "KXTRUMPAPPROVALYEAR", "KXTRUMPPRES", "KXTRUMPRUN",
    "KXIMPEACH", "KXMARTIAL", "KXNEXTPRESSEC", "KXNEXTDHSSEC",
    "KXDEBTGROWTH", "KXACAREPEAL", "KXFREEIVF", "KXTAFTHARTLEY",
    "KXBALANCEPOWERCOMBO", "KXCAPCONTROL", "KXDOED",
    "KXSCOTUSPOWER", "KXJAN6CASES", "KXOBERGEFELL",
    "KXUSDEBT", "KXLCPIMAXYOY",
    "KXFEDCHAIRNOM", "KXFEDEMPLOYEES", "KXTRILLIONAIRE",
    "KXKHAMENEIOUT", "KXGREENTERRITORY", "KXGREENLANDPRICE",
    "KXCANAL", "KXNEWPOPE", "KXFULLTERMSKPRES",
    "KXNEXTIRANLEADER", "KXPUTINDJTLOCATION",
    "KXWITHDRAW", "KXUSAKIM", "KXRECOGSOMALI",
    "KXFTA", "KXDJTVOSTARIFFS", "KXZELENSKYPUTIN",
    "KXPRESNOMD", "KXVPRESNOMD", "KXPRESPARTY",
    "KXHOUSERACE", "KXMUSKPRIMARY", "KXAOCSENATE",
    "CONTROLH", "POWER",
    "KXIPOSPACEX", "KXIPOFANNIE", "KXSPACEXBANKPUBLIC",
    "KXTARIFFS",
    "KXBILLSIGNED", "KXTRUMPBILLSSIGNED",
]

def api_get(endpoint, params=None):
    try:
        resp = requests.get(f"{API_BASE}{endpoint}", params=params, timeout=15)
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        # print(f"API error on {endpoint}: {e}")  # Suppress noise
        return None

def fetch_market_details(ticker):
    """Fetch complete market details including rules"""
    data = api_get(f"/markets/{ticker}")
    if not data:
        return None
    return data.get("market", {})

def kalshi_url(ticker):
    return f"https://kalshi.com/markets/{ticker.lower()}"

def search_polymarket(query, max_results=3):
    """Search Polymarket for matching markets, return YES probability (0-1) or None"""
    try:
        r = requests.get("https://gamma-api.polymarket.com/events",
                        params={"active": "true", "closed": "false", "limit": 20},
                        timeout=10)
        if r.status_code != 200:
            return None
        data = r.json()
        events = data if isinstance(data, list) else []
        # Fuzzy match by title keywords
        query_lower = query.lower()
        for event in events:
            title = event.get("title", "").lower()
            # Check if 2+ keywords match
            keywords = [w for w in query_lower.split() if len(w) > 3]
            matches = sum(1 for kw in keywords if kw in title)
            if matches >= 2:
                markets = event.get("markets", [])
                if markets:
                    # Get the main market price (0-1 range)
                    prices = json.loads(markets[0].get("outcomePrices", "[]"))
                    if prices:
                        return float(prices[0])
        return None
    except:
        return None


def search_news(query, max_results=5):
    """Search Google News RSS for recent articles"""
    results = []
    try:
        import urllib.parse
        query_encoded = urllib.parse.quote(query)
        url = f"https://news.google.com/rss/search?q={query_encoded}&hl=en-US&gl=US&ceid=US:en"
        
        if hasattr(requests, 'get'):
            # Using requests
            r = requests.get(url, timeout=10)
            text = r.text
        else:
            # Using urllib fallback
            import urllib.request
            req = urllib.request.Request(url)
            with urllib.request.urlopen(req, timeout=10) as resp:
                text = resp.read().decode('utf-8')
        
        import re
        titles = re.findall(r'<title>(.*?)</title>', text)
        dates = re.findall(r'<pubDate>(.*?)</pubDate>', text)
        
        for i, title in enumerate(titles[1:max_results+1]):  # skip feed title
            results.append({
                "title": title,
                "date": dates[i] if i < len(dates) else "",
            })
    except Exception as e:
        # Fail silently, don't block on news errors
        pass
    return results

def format_vol(v):
    if v >= 1_000_000:
        return f"{v/1_000_000:.1f}M"
    elif v >= 1_000:
        return f"{v/1_000:.0f}K"
    return str(v)

def analyze_rules(rules_text):
    """Parse resolution rules"""
    analysis = {
        "official_source": None,
        "procedural_risk": False,
        "time_window": None,
        "ambiguity": False,
    }
    
    if not rules_text:
        analysis["ambiguity"] = True
        return analysis
    
    text_lower = rules_text.lower()
    
    # Official data sources
    sources = {
        "BEA": ["bureau of economic analysis", "bea.gov", "bea's", " bea ", "gdp release"],
        "BLS": ["bureau of labor statistics", "bls.gov", "bls's", " bls ", "cpi release"],
        "Fed": ["federal reserve", "fomc", "fed.gov", "interest rate decision"],
        "Congress": ["congress.gov", "congressional", "legislative", "house.gov", "senate.gov"],
        "White House": ["whitehouse.gov", "executive order", "presidential", "president signs"],
        "Treasury": ["treasury.gov", "treasury department"],
    }
    
    for source, keywords in sources.items():
        if any(kw in text_lower for kw in keywords):
            analysis["official_source"] = source
            break
    
    # Implicit sources (inferred from indicators mentioned in rules)
    if not analysis["official_source"]:
        # CPI â†’ BLS
        if "consumer price index" in text_lower or " cpi " in text_lower:
            analysis["official_source"] = "BLS"
        # GDP â†’ BEA
        elif " gdp " in text_lower or "gross domestic product" in text_lower or "real gdp" in text_lower:
            analysis["official_source"] = "BEA"
        # Unemployment â†’ BLS
        elif "unemployment" in text_lower or "jobs report" in text_lower:
            analysis["official_source"] = "BLS"
        # Interest rate â†’ Fed
        elif "interest rate" in text_lower or "federal funds rate" in text_lower:
            analysis["official_source"] = "Fed"
    
    # Procedural complexity
    procedural_keywords = [
        "pass both", "senate and house", "signed into law",
        "confirmed by", "ratified", "approved by congress",
    ]
    if any(kw in text_lower for kw in procedural_keywords):
        analysis["procedural_risk"] = True
    
    # Ambiguous terms
    ambiguous_terms = ["may", "could", "might", "approximately", "around"]
    if any(term in text_lower for term in ambiguous_terms):
        analysis["ambiguity"] = True
    
    return analysis

def score_market(m):
    """Score and decide on a market"""
    score = 0
    reasons = []
    
    price = m.get("last_price", 50)
    spread = (m.get("yes_ask", 0) - m.get("yes_bid", 0)) if m.get("yes_ask") else 99
    # Combine primary and secondary rules
    rules_primary = m.get("rules_primary", "")
    rules_secondary = m.get("rules_secondary", "")
    rules = f"{rules_primary} {rules_secondary}"
    
    close_str = m.get("close_time", "")
    if not close_str:
        return None
    
    try:
        close = datetime.fromisoformat(close_str.replace("Z", "+00:00"))
        days = (close - datetime.now(timezone.utc)).days
    except:
        return None
    
    if days <= 0:
        return None
    
    side = "YES" if price >= 85 else "NO"
    cost = price if price >= 85 else (100 - price)
    ret = ((100 - cost) / cost) * 100 if cost > 0 else 0
    ann_yield = (ret / max(days, 1)) * 365
    
    if ann_yield < 100:
        return None
    
    score += int(ann_yield / 100) * 10
    reasons.append(f"å¹´åŒ– {ann_yield:.0f}%")
    
    if spread <= 3:
        score += 10
        reasons.append("æµåŠ¨æ€§å¥½")
    elif spread <= 5:
        score += 5
        reasons.append("æµåŠ¨æ€§å°šå¯")
    
    rule_analysis = analyze_rules(rules)
    
    if rule_analysis["official_source"]:
        score += 30
        reasons.append(f"âœ… {rule_analysis['official_source']} æ•°æ®æº")
    else:
        reasons.append("âš ï¸ æ— å®˜æ–¹æ•°æ®æº")
    
    if not rule_analysis["procedural_risk"]:
        score += 20
        reasons.append("âœ… æ— ç¨‹åºæ€§é£é™©")
    else:
        reasons.append("âš ï¸ æœ‰ç¨‹åºæ€§éšœç¢")
    
    if rule_analysis["ambiguity"]:
        score -= 10
        reasons.append("âš ï¸ è§„åˆ™æ¨¡ç³Š")
    
    # News validation (only for promising candidates)
    news_count = 0
    pm_price_val = None
    if score >= 40:  # Only search news for candidates worth considering
        title = m.get("title", "")
        subtitle = m.get("yes_sub_title", "") or m.get("no_sub_title", "")
        
        # Build search query from title keywords
        query_terms = []
        # Extract key terms from title
        import re
        # Clean markdown and special characters first
        title_clean = re.sub(r'\*\*', '', title)  # Remove markdown bold
        title_clean = re.sub(r'[^\w\s]', ' ', title_clean)  # Remove punctuation
        # Remove common words and extract important terms
        title_clean = re.sub(r'\b(will|the|a|an|in|on|at|to|for|of|by|more|than|less|increase|decrease)\b', '', title_clean.lower())
        terms = [t.strip() for t in title_clean.split() if len(t.strip()) > 2][:3]
        query_terms.extend(terms)
        
        if query_terms:
            query = " ".join(query_terms)
            news_results = search_news(query, max_results=5)
            news_count = len([n for n in news_results if n.get("title")])
            
            if news_count >= 3:
                score += 20
                reasons.append(f"âœ… {news_count} æ¡ç›¸å…³æ–°é—»")
            elif news_count > 0:
                score += 10
                reasons.append(f"âš ï¸ ä»… {news_count} æ¡æ–°é—»")
            else:
                reasons.append("âŒ æ— ç›¸å…³æ–°é—»")
        
        # Small delay to avoid rate limiting on news API
        time.sleep(0.1)

        # Polymarket cross-validation
        if query_terms:
            pm_price_val = search_polymarket(query)
            if pm_price_val is not None:
                kalshi_prob = price / 100  # YES price as probability
                gap = abs(pm_price_val - kalshi_prob)
                if gap < 0.05:  # Prices agree
                    score += 15
                    reasons.append(f"âœ… Polymarket {pm_price_val:.0%} ä¸€è‡´")
                elif gap > 0.15:  # Big divergence = potential arb
                    score += 5
                    reasons.append(f"âš ï¸ Polymarket {pm_price_val:.0%} åå·®å¤§ (gap {gap:.0%})")

    # Decision
    if score >= 70:
        decision = "ğŸŸ¢ BUY"
        confidence = "HIGH"
        position = 200 if score >= 85 else 100
    elif score >= 50:
        decision = "ğŸŸ¡ WAIT"
        confidence = "MEDIUM"
        position = 50
    else:
        decision = "ğŸ”´ SKIP"
        confidence = "LOW"
        position = 0
    
    return {
        "decision": decision,
        "score": score,
        "confidence": confidence,
        "position": position,
        "side": side,
        "cost": cost,
        "ann_yield": ann_yield,
        "days": days,
        "reasons": reasons,
        "spread": spread,
        "vol": m.get("volume_24h", 0),
        "ticker": m.get("ticker", ""),
        "title": m.get("title", ""),
        "sub": m.get("yes_sub_title", "") or m.get("no_sub_title", ""),
        "pm_price": pm_price_val,
    }

def scan_and_decide():
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    now = datetime.now(timezone.utc)
    
    # === OPTIMIZATION CONFIG ===
    MIN_VOLUME = 200  # Skip low liquidity markets
    MAX_WORKERS_SERIES = 3   # Conservative: Kalshi rate-limits at 8+ concurrent (429)
    MAX_WORKERS_DETAILS = 5  # Parallel detail fetches
    
    # Step 1: Fetch all political markets (PARALLEL with rate-limit safety)
    print(f"Scanning {len(POLITICAL_SERIES)} series (parallel, 3 workers)...", file=sys.stderr, flush=True)
    all_markets = []
    
    def fetch_series(series):
        """Fetch series with retry on 429 rate-limit."""
        import requests as _req
        for attempt in range(3):
            try:
                resp = _req.get(
                    f"{API_BASE}/markets",
                    params={"limit": 50, "status": "open", "series_ticker": series},
                    timeout=15,
                )
                if resp.status_code == 429:
                    time.sleep(1.5 * (attempt + 1))  # backoff: 1.5s, 3s, 4.5s
                    continue
                if resp.status_code == 200:
                    return resp.json().get("markets", [])
                return []
            except Exception:
                time.sleep(0.5)
        return []
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS_SERIES) as executor:
        futures = {executor.submit(fetch_series, s): s for s in POLITICAL_SERIES}
        done = 0
        for future in as_completed(futures):
            markets = future.result()
            all_markets.extend(markets)
            done += 1
            if done % 20 == 0:
                print(f"  Progress: {done}/{len(POLITICAL_SERIES)} series, {len(all_markets)} markets", file=sys.stderr, flush=True)
    
    # Step 2: Filter candidates (extreme price + volume filter)
    candidates = []
    filtered_low_vol = 0
    for m in all_markets:
        price = m.get("last_price", 50)
        volume = m.get("volume_24h", 0) or m.get("volume", 0)
        
        # Skip low volume markets (optimization)
        if volume < MIN_VOLUME:
            filtered_low_vol += 1
            continue
            
        if (price >= 85 or price <= 12):
            candidates.append(m)
    
    print(f"Found {len(candidates)} candidates from {len(all_markets)} markets (filtered {filtered_low_vol} low-vol)", file=sys.stderr)
    
    # Step 3: Fetch detailed rules (PARALLEL)
    print(f"Analyzing {len(candidates)} candidates (parallel)...", file=sys.stderr, flush=True)
    
    def analyze_candidate(m):
        ticker = m.get("ticker", "")
        detailed = fetch_market_details(ticker)
        if detailed:
            m["rules_primary"] = detailed.get("rules_primary", "")
            m["rules_secondary"] = detailed.get("rules_secondary", "")
            return score_market(m)
        return None
    
    opportunities = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS_DETAILS) as executor:
        futures = {executor.submit(analyze_candidate, m): m for m in candidates}
        done = 0
        for future in as_completed(futures):
            result = future.result()
            if result:
                opportunities.append(result)
            done += 1
            if done % 20 == 0 or done == len(candidates):
                print(f"  Progress: {done}/{len(candidates)} analyzed", file=sys.stderr, flush=True)
    
    # Sort by score
    opportunities.sort(key=lambda x: -x["score"])
    
    # Load existing positions from both accounts
    positions_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), "positions.json")
    existing_positions = {}  # ticker -> {side, qty, account}
    try:
        with open(positions_file, "r") as f:
            pos_data = json.load(f)
            for p in pos_data.get("positions", []):
                ticker = p.get("ticker", "")
                existing_positions[ticker] = {
                    "side": p.get("side"),
                    "qty": p.get("contracts", 0),
                    "account": p.get("account", "ä¸»è´¦å·"),
                    "entry": p.get("entry_price", 0)
                }
    except Exception as e:
        print(f"âš ï¸ Could not load positions: {e}", file=sys.stderr)
    
    # Format report
    lines = []
    lines.append(f"âš¡ Kalshi Decision Report â€” {now.strftime('%m/%d %H:%M UTC')}")
    lines.append(f"æ‰«æäº† {len(all_markets)} ä¸ªå¸‚åœºï¼Œæ‰¾åˆ° {len(opportunities)} ä¸ªé«˜ç¡®å®šæ€§æœºä¼š\n")
    
    if not opportunities:
        lines.append("ğŸ˜´ æš‚æ— ç¬¦åˆæ ‡å‡†çš„æœºä¼š")
        return "\n".join(lines)
    
    # Categorize
    buys = [o for o in opportunities if "BUY" in o["decision"]]
    waits = [o for o in opportunities if "WAIT" in o["decision"]]
    skips = [o for o in opportunities if "SKIP" in o["decision"]]
    
    # BUY recommendations
    if buys:
        lines.append(f"ğŸŸ¢ æ¨èä¹°å…¥ ({len(buys)})\n")
        for i, o in enumerate(buys[:5], 1):
            full_name = f"{o['title']} â†’ {o['sub']}" if o['sub'] else o['title']
            ticker = o['ticker']
            
            # Check if already have position
            pos_info = existing_positions.get(ticker)
            pos_tag = ""
            if pos_info:
                pos_tag = f" ğŸ“Œ å·²æŒæœ‰ {pos_info['qty']}å¼ {pos_info['side']}@{pos_info['entry']}Â¢ ({pos_info['account']})"
            
            pm_tag = f" | PM {o['pm_price']:.0%}" if o.get("pm_price") is not None else ""
            lines.append(f"#{i} {o['decision']} â€” è¯„åˆ† {o['score']}/100{pos_tag}")
            lines.append(f"   {full_name}")
            lines.append(f"   ğŸ‘‰ {o['side']} @ {o['cost']:.0f}Â¢ | ä»“ä½ ${o['position']}{pm_tag}")
            lines.append(f"   ğŸ“Š {o['ann_yield']:.0f}% å¹´åŒ– ({o['days']}å¤©) | spread {o['spread']}Â¢ | é‡ {format_vol(o['vol'])}")
            lines.append(f"   ğŸ’¡ {' | '.join(o['reasons'])}")
            lines.append(f"   ğŸ”— {kalshi_url(o['ticker'])}\n")
    
    # WAIT candidates
    if waits:
        lines.append(f"ğŸŸ¡ è§‚æœ›ä¸­ ({len(waits)})\n")
        for o in waits[:3]:
            full_name = f"{o['title']} â†’ {o['sub']}" if o['sub'] else o['title']
            pm_tag = f" | PM {o['pm_price']:.0%}" if o.get("pm_price") is not None else ""
            lines.append(f"   {o['decision']} ({o['score']}/100) â€” {full_name}")
            lines.append(f"   {o['side']} @ {o['cost']:.0f}Â¢ | {o['ann_yield']:.0f}% ann{pm_tag}")
            lines.append(f"   ğŸ’¡ {' | '.join(o['reasons'])}")
            lines.append(f"   {kalshi_url(o['ticker'])}\n")
    
    # SKIP (show why they were rejected)
    if skips and not buys and not waits:
        lines.append(f"ğŸ”´ å·²æ‹’ç» ({len(skips)}) â€” é«˜æ”¶ç›Šä½†é£é™©ä¸å¯æ§\n")
        for o in skips[:3]:
            full_name = f"{o['title']} â†’ {o['sub']}" if o['sub'] else o['title']
            lines.append(f"   SKIP ({o['score']}/100) â€” {full_name}")
            lines.append(f"   {o['side']} @ {o['cost']:.0f}Â¢ | {o['ann_yield']:.0f}% ann")
            lines.append(f"   âŒ æ‹’ç»åŸå› : {' | '.join(o['reasons'])}")
            lines.append(f"   {kalshi_url(o['ticker'])}\n")
    
    if not buys and not waits:
        lines.append("\nâš ï¸ æœ¬è½®æ‰«ææ— æ¨èæ ‡çš„ â€” æ‰€æœ‰é«˜æ”¶ç›Šæœºä¼šéƒ½å› è§„åˆ™/æ•°æ®æºé—®é¢˜è¢«æ‹’ç»")
    
    return "\n".join(lines)

if __name__ == "__main__":
    report = scan_and_decide()
    print(report)
